Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2018-08-10T11:34:16-04:00

====== DenseNet ======

https://arxiv.org/pdf/1608.06993.pdf

Densenet is a network architecture published by Cornell, Tsinghua, and Facebook on January 28th, 2018. Its [[Layers]] each connect to all previous layers rather than simply the immediately previous layer. This solves a lot of problems and results in state of the art performance. From the abstract:

Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output.  In this paper, we embrace this observation and introduce the Dense Convolutional  Network  (DenseNet), which  connects  each  layer to  every  other  layer  in  a  feed-forward  fashion.   Whereas traditional  convolutional  networks  with layers  have connections—one between  each  layer  and  its  subsequent layer—our  network  has L(L+1)/2 direct  connections. For each  layer,  the  feature-maps  of  all  preceding  layers  are used as inputs, and its own feature-maps are used as inputs into  all  subsequent  layers.   DenseNets  have  several  compelling  advantages:  they  alleviate  the vanishing-gradient problem,  strengthen  feature  propagation,  encourage  fea- ture reuse, and substantially reduce the number of parameters.  We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).  DenseNets obtain significant improvements over the state-of-the-art on most ofthem, whilst requiring less computation to achieve high performance.   Code  and  pre-trained  models  are available  at https://github.com/liuzhuang13/DenseNet.
